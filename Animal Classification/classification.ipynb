{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15180afd-d632-4143-b4c9-4a9d6387a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc32ce25-4271-4fd0-bb04-563fd959a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    " import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import clip \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "766bf772-f16c-4765-a8d5-ec04b56f1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0a5f45-bcd1-4be4-a633-b2548fdea700",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalsDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        # sorted for consistency\n",
    "        classes = sorted(os.listdir(root_dir))\n",
    "        for idx, cls in enumerate(classes):\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(cls_path):\n",
    "                continue\n",
    "\n",
    "            self.class_to_idx[cls] = idx\n",
    "\n",
    "            for file in os.listdir(cls_path):\n",
    "                if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.image_paths.append(os.path.join(cls_path, file))\n",
    "                    self.labels.append(idx)\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} images across {len(self.class_to_idx)} classes.\")\n",
    "        print(\"Class mapping:\", self.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf88726-5489-4795-a823-36724e162252",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fdc7507-175c-4537-99de-829009ecca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:11<00:00, 31.4MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)  # downloads if needed\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46122c7e-dba5-4904-9078-bc5b69fc5a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 498 images across 6 classes.\n",
      "Class mapping: {'butterfly': 0, 'cat': 1, 'dog': 2, 'elephant': 3, 'horse': 4, 'squirrel': 5}\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/home/akash/Downloads/Animals-10\"\n",
    "dataset = AnimalsDataset(root_dir=root_dir, transform=preprocess)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,      # important: we want features aligned with labels\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e658f9d2-43a2-45bf-9229-000ba45728e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.eval()\n",
    "\n",
    "class AnimalImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        classes = sorted(os.listdir(root_dir))\n",
    "        for idx, cls in enumerate(classes):\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(cls_path):\n",
    "                continue\n",
    "\n",
    "            self.class_to_idx[cls] = idx\n",
    "\n",
    "            for file in os.listdir(cls_path):\n",
    "                if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    self.image_paths.append(os.path.join(cls_path, file))\n",
    "                    self.labels.append(idx)\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} images across {len(self.class_to_idx)} classes.\")\n",
    "        print(\"Class mapping:\", self.class_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        # <- THIS is critical, must be at same indent level as __init__\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "692db0e0-b9a4-4b40-9c56-4cb517df4b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 498 images across 6 classes.\n",
      "Class mapping: {'butterfly': 0, 'cat': 1, 'dog': 2, 'elephant': 3, 'horse': 4, 'squirrel': 5}\n",
      "len(dataset) = 498\n",
      "Batch images shape: torch.Size([64, 3, 224, 224])\n",
      "Batch labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"/home/akash/Downloads/Animals-10\"\n",
    "\n",
    "dataset = AnimalImageDataset(\n",
    "    root_dir=root_dir,\n",
    "    transform=preprocess\n",
    ")\n",
    "\n",
    "print(\"len(dataset) =\", len(dataset))  # should print 498 or similar\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# quick test: just grab one batch\n",
    "for images, labels in loader:\n",
    "    print(\"Batch images shape:\", images.shape)\n",
    "    print(\"Batch labels shape:\", labels.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1421814-6b18-48b7-8c68-0b70bbc465f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (498, 512)\n",
      "Labels shape: (498,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Encode using CLIP\n",
    "        feats = model.encode_image(images)          # [batch, 512]\n",
    "        feats = feats / feats.norm(dim=-1, keepdim=True)  # L2 normalize\n",
    "\n",
    "        all_features.append(feats.cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "all_features = torch.cat(all_features, dim=0).numpy()   # (N, 512)\n",
    "all_labels = torch.cat(all_labels, dim=0).numpy()       # (N,)\n",
    "\n",
    "print(\"Embeddings shape:\", all_features.shape)\n",
    "print(\"Labels shape:\", all_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77683e89-02b2-48e4-b668-611f687ba65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (398, 512) Test: (100, 512)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_features,\n",
    "    all_labels,\n",
    "    test_size=0.2,        # 20% test\n",
    "    stratify=all_labels,  # keep class balance\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af0f6b6-007b-4737-8ad2-c43b36d42c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/miniconda3/envs/ml/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    reader_close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self._close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    self._close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 953, in run\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    self.run()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7f2dbabf5090>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 132, in __del__\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 239, in _feed\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 240, in _feed\n",
      "    reader_close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    self._close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    # Message can fit in dest\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    \"\"\"Release a lock, decrementing the recursion level.\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    reader_close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 177, in close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self._close()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/connection.py\", line 361, in _close\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 953, in run\n",
      "    self.run()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "    self.run()\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 772, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/akash/miniconda3/envs/ml/lib/python3.10/multiprocessing/queues.py\", line 271, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression ===\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        28\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       1.00      1.00      1.00        10\n",
      "           3       1.00      1.00      1.00        22\n",
      "           4       1.00      1.00      1.00        10\n",
      "           5       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "\n",
      "=== KNN (k=5, cosine) ===\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        28\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       1.00      1.00      1.00        10\n",
      "           3       1.00      1.00      1.00        22\n",
      "           4       1.00      1.00      1.00        10\n",
      "           5       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n",
      "\n",
      "=== Gaussian Naive Bayes ===\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        28\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       1.00      1.00      1.00        10\n",
      "           3       1.00      1.00      1.00        22\n",
      "           4       1.00      1.00      1.00        10\n",
      "           5       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00       100\n",
      "   macro avg       1.00      1.00      1.00       100\n",
      "weighted avg       1.00      1.00      1.00       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- Logistic Regression ---\n",
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,\n",
    "    multi_class=\"multinomial\"\n",
    ")\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "print(\"=== Logistic Regression ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "\n",
    "# --- KNN (k=5, cosine distance) ---\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    metric=\"cosine\"\n",
    ")\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "print(\"\\n=== KNN (k=5, cosine) ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "\n",
    "\n",
    "# --- Gaussian Naive Bayes ---\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred_nb = nb.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Gaussian Naive Bayes ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddbfc175-9001-4dfc-a286-74f977e21c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved!\n",
      "Class names saved: ['butterfly', 'cat', 'dog', 'elephant', 'horse', 'squirrel']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Make an output folder (optional, but tidy)\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# 1. Save the three classifiers\n",
    "joblib.dump(log_reg, \"saved_models/logreg_clip_animals.pkl\")\n",
    "joblib.dump(knn, \"saved_models/knn_clip_animals.pkl\")\n",
    "joblib.dump(nb, \"saved_models/nb_clip_animals.pkl\")\n",
    "\n",
    "print(\"Models saved!\")\n",
    "\n",
    "# 2. Save class names in index order: index -> class name\n",
    "class_names = [None] * len(dataset.class_to_idx)\n",
    "for cls_name, idx in dataset.class_to_idx.items():\n",
    "    class_names[idx] = cls_name\n",
    "\n",
    "joblib.dump(class_names, \"saved_models/class_names.pkl\")\n",
    "print(\"Class names saved:\", class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c79b3-7034-401a-b96f-8cff4ae9bff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
